{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74d85497",
   "metadata": {},
   "source": [
    "# Simple modal to predict if machine is working or not, after given number of hours it has been active\n",
    "\n",
    "The training data should have labels of active working hours of similar machine and wether its working or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "977849b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224d71b7",
   "metadata": {},
   "source": [
    "Each row in the 2D tensor corresponds to a single data sample, and the columns correspond to the features of that sample. \n",
    "\n",
    "*Other parameters on makin a tensor*\n",
    "- shape (or size()): Dimensions of the tensor.\n",
    "- ndimension(): Number of dimensions (axes) the tensor has.\n",
    "- numel(): Total number of elements in the tensor.\n",
    "- is_cuda: Returns True if tensor is on a CUDA device (GPU).\n",
    "- stride(): Returns the stride of the tensor (how far apart elements are).\n",
    "- dim(): Number of dimensions (like ndimension()).\n",
    "- flatten(): Flattens the tensor into a 1D tensor.\n",
    "- view(): Reshapes the tensor to a new shape.\n",
    "- transpose() or t(): Transposes a 2D tensor (swap rows and columns).\n",
    "- clone(): Returns a copy of the tensor with the same content but different memory allocation.\n",
    "- to(): Moves a tensor to another device or changes its data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9242a518-5e64-426b-a26c-b54f0ce4a982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset (feature: hours used)\n",
    "# Labels: [0 = broken, 1 = working]\n",
    "data = torch.tensor([\n",
    "    [100],\n",
    "    [0],\n",
    "    [50],\n",
    "    [900],\n",
    "    [10],\n",
    "    [3],\n",
    "    [700],\n",
    "                     [200], \n",
    "                     [1500], \n",
    "                     [800], \n",
    "                     [1200], \n",
    "                     [400]],\n",
    "                   dtype=torch.float32)\n",
    "\n",
    "labels = torch.tensor([\n",
    "    0,\n",
    "    1,\n",
    "    1,\n",
    "    0,\n",
    "    1,\n",
    "    1,\n",
    "    0,\n",
    "    1, 0, 1, 0, 1], dtype=torch.float32).view(-1, 1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68cd66fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: tensor([[ 100.],\n",
      "        [   0.],\n",
      "        [  50.],\n",
      "        [ 900.],\n",
      "        [  10.],\n",
      "        [   3.],\n",
      "        [ 700.],\n",
      "        [ 200.],\n",
      "        [1500.],\n",
      "        [ 800.],\n",
      "        [1200.],\n",
      "        [ 400.]])\n",
      "Labels: tensor([[0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "# Checking the shapes of data and labels\n",
    "print(\"Data:\", data)\n",
    "print(\"Labels:\", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56efaeb-0fbe-4f42-8ded-f2195ae72d71",
   "metadata": {},
   "source": [
    "# Defining a simple model with Linear Layer, Activation, and Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bfeafff-a157-404f-8f5a-c2bdeb5c2ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MachineStatusModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MachineStatusModel, self).__init__()\n",
    "        \n",
    "        # the linear layer\n",
    "        # Input size = 1 (hours used) and the output size = 1 (probability of working)\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "        \n",
    "        # Layer normalization and is applied after linear layer\n",
    "        self.layer_norm = nn.LayerNorm(1)\n",
    "        \n",
    "        # This is Sigmoid activation function \n",
    "        #Is used for binary classification : (0 = broken, 1 = working)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Data flows through the network\n",
    "        # Linear transformation: W*x + b\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        # Apply Layer Normalization\n",
    "        x = self.layer_norm(x)\n",
    "        \n",
    "        # Apply Sigmoid to get the probability of machine working (between 0 and 1)\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81937a61-44ac-4f71-a3da-b89d6fd19b20",
   "metadata": {},
   "source": [
    "# Instantiating the model, loss function, and optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2ad910",
   "metadata": {},
   "source": [
    "nn.BCELoss(), BCELoss stands for Binary Cross-Entropy Loss, which is commonly used for binary classification tasks where the output is a probability value (i.e., between 0 and 1)\n",
    "\n",
    "This loss function measures how well the model's predicted probabilities match the true binary labels (0 or 1). The output of a binary classifier is usually passed through a sigmoid activation function to map it to a probability between 0 and 1.\n",
    "\n",
    "---\n",
    "\n",
    "optim.SGD() , is the Stochastic Gradient Descent (SGD) optimizer. It's a commonly used optimization algorithm that updates model parameters based on the gradient of the loss function with respect to those parameters.\n",
    "\n",
    "model.parameters() passes all the trainable parameters of the MachineStatusModel() to the optimizer.\n",
    "\n",
    "lr=0.01 sets the learning rate for the optimizer. The learning rate determines the step size at each iteration while moving towards a minimum of the loss function. \n",
    "A small learning rate (e.g., 0.01) means the model will learn more slowly but can converge more precisely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a931660b-2375-4823-9fa0-0db54b9c9210",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MachineStatusModel()\n",
    "# Binary Cross-Entropy Loss (since this is a binary classification problem)\n",
    "criterion = nn.BCELoss()\n",
    "# Stochastic Gradient Descent (SGD) for optimization\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd089b66-4396-4a99-8df7-c06483939818",
   "metadata": {},
   "source": [
    "# Training the model with given data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee73c015-fada-4aaa-abd2-a8b2e7540dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/10000], Loss: 0.6792\n",
      "Epoch [200/10000], Loss: 0.6792\n",
      "Epoch [300/10000], Loss: 0.6792\n",
      "Epoch [400/10000], Loss: 0.6792\n",
      "Epoch [500/10000], Loss: 0.6792\n",
      "Epoch [600/10000], Loss: 0.6792\n",
      "Epoch [700/10000], Loss: 0.6792\n",
      "Epoch [800/10000], Loss: 0.6792\n",
      "Epoch [900/10000], Loss: 0.6792\n",
      "Epoch [1000/10000], Loss: 0.6792\n",
      "Epoch [1100/10000], Loss: 0.6792\n",
      "Epoch [1200/10000], Loss: 0.6792\n",
      "Epoch [1300/10000], Loss: 0.6792\n",
      "Epoch [1400/10000], Loss: 0.6792\n",
      "Epoch [1500/10000], Loss: 0.6792\n",
      "Epoch [1600/10000], Loss: 0.6792\n",
      "Epoch [1700/10000], Loss: 0.6792\n",
      "Epoch [1800/10000], Loss: 0.6792\n",
      "Epoch [1900/10000], Loss: 0.6792\n",
      "Epoch [2000/10000], Loss: 0.6792\n",
      "Epoch [2100/10000], Loss: 0.6792\n",
      "Epoch [2200/10000], Loss: 0.6792\n",
      "Epoch [2300/10000], Loss: 0.6792\n",
      "Epoch [2400/10000], Loss: 0.6792\n",
      "Epoch [2500/10000], Loss: 0.6792\n",
      "Epoch [2600/10000], Loss: 0.6792\n",
      "Epoch [2700/10000], Loss: 0.6792\n",
      "Epoch [2800/10000], Loss: 0.6792\n",
      "Epoch [2900/10000], Loss: 0.6792\n",
      "Epoch [3000/10000], Loss: 0.6792\n",
      "Epoch [3100/10000], Loss: 0.6792\n",
      "Epoch [3200/10000], Loss: 0.6792\n",
      "Epoch [3300/10000], Loss: 0.6792\n",
      "Epoch [3400/10000], Loss: 0.6792\n",
      "Epoch [3500/10000], Loss: 0.6792\n",
      "Epoch [3600/10000], Loss: 0.6792\n",
      "Epoch [3700/10000], Loss: 0.6792\n",
      "Epoch [3800/10000], Loss: 0.6792\n",
      "Epoch [3900/10000], Loss: 0.6792\n",
      "Epoch [4000/10000], Loss: 0.6792\n",
      "Epoch [4100/10000], Loss: 0.6792\n",
      "Epoch [4200/10000], Loss: 0.6792\n",
      "Epoch [4300/10000], Loss: 0.6792\n",
      "Epoch [4400/10000], Loss: 0.6792\n",
      "Epoch [4500/10000], Loss: 0.6792\n",
      "Epoch [4600/10000], Loss: 0.6792\n",
      "Epoch [4700/10000], Loss: 0.6792\n",
      "Epoch [4800/10000], Loss: 0.6792\n",
      "Epoch [4900/10000], Loss: 0.6792\n",
      "Epoch [5000/10000], Loss: 0.6792\n",
      "Epoch [5100/10000], Loss: 0.6792\n",
      "Epoch [5200/10000], Loss: 0.6792\n",
      "Epoch [5300/10000], Loss: 0.6792\n",
      "Epoch [5400/10000], Loss: 0.6792\n",
      "Epoch [5500/10000], Loss: 0.6792\n",
      "Epoch [5600/10000], Loss: 0.6792\n",
      "Epoch [5700/10000], Loss: 0.6792\n",
      "Epoch [5800/10000], Loss: 0.6792\n",
      "Epoch [5900/10000], Loss: 0.6792\n",
      "Epoch [6000/10000], Loss: 0.6792\n",
      "Epoch [6100/10000], Loss: 0.6792\n",
      "Epoch [6200/10000], Loss: 0.6792\n",
      "Epoch [6300/10000], Loss: 0.6792\n",
      "Epoch [6400/10000], Loss: 0.6792\n",
      "Epoch [6500/10000], Loss: 0.6792\n",
      "Epoch [6600/10000], Loss: 0.6792\n",
      "Epoch [6700/10000], Loss: 0.6792\n",
      "Epoch [6800/10000], Loss: 0.6792\n",
      "Epoch [6900/10000], Loss: 0.6792\n",
      "Epoch [7000/10000], Loss: 0.6792\n",
      "Epoch [7100/10000], Loss: 0.6792\n",
      "Epoch [7200/10000], Loss: 0.6792\n",
      "Epoch [7300/10000], Loss: 0.6792\n",
      "Epoch [7400/10000], Loss: 0.6792\n",
      "Epoch [7500/10000], Loss: 0.6792\n",
      "Epoch [7600/10000], Loss: 0.6792\n",
      "Epoch [7700/10000], Loss: 0.6792\n",
      "Epoch [7800/10000], Loss: 0.6792\n",
      "Epoch [7900/10000], Loss: 0.6792\n",
      "Epoch [8000/10000], Loss: 0.6792\n",
      "Epoch [8100/10000], Loss: 0.6792\n",
      "Epoch [8200/10000], Loss: 0.6792\n",
      "Epoch [8300/10000], Loss: 0.6792\n",
      "Epoch [8400/10000], Loss: 0.6792\n",
      "Epoch [8500/10000], Loss: 0.6792\n",
      "Epoch [8600/10000], Loss: 0.6792\n",
      "Epoch [8700/10000], Loss: 0.6792\n",
      "Epoch [8800/10000], Loss: 0.6792\n",
      "Epoch [8900/10000], Loss: 0.6792\n",
      "Epoch [9000/10000], Loss: 0.6792\n",
      "Epoch [9100/10000], Loss: 0.6792\n",
      "Epoch [9200/10000], Loss: 0.6792\n",
      "Epoch [9300/10000], Loss: 0.6792\n",
      "Epoch [9400/10000], Loss: 0.6792\n",
      "Epoch [9500/10000], Loss: 0.6792\n",
      "Epoch [9600/10000], Loss: 0.6792\n",
      "Epoch [9700/10000], Loss: 0.6792\n",
      "Epoch [9800/10000], Loss: 0.6792\n",
      "Epoch [9900/10000], Loss: 0.6792\n",
      "Epoch [10000/10000], Loss: 0.6792\n"
     ]
    }
   ],
   "source": [
    "epochs = 10000\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    # compute the model output (probability of working)\n",
    "    predictions = model(data)\n",
    "    \n",
    "    # finding the loss by passing predictions and expected labels\n",
    "    loss = criterion(predictions, labels)\n",
    "    \n",
    "\n",
    "    # Backward pass: \n",
    "    # compute gradients\n",
    "    # Clear previous gradients\n",
    "    optimizer.zero_grad()  \n",
    "    # New gradients\n",
    "    loss.backward()  \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # Update weights using the optimizer\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print the loss every 100 epochs to track progress\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5240247b-53f8-4c99-b594-95af75237d80",
   "metadata": {},
   "source": [
    "# Evaluating the trained model\n",
    "\n",
    "to get an output from a simple input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f8065cf-56f9-41a2-b8db-a875af29142f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted probability of working for 900 hours of usage: 0.5833\n",
      "Predicted class (0=Broken, 1=Working): 1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    # Now let's predict\n",
    "    new_machine = torch.tensor([[10]], dtype=torch.float32)\n",
    "    \n",
    "    predicted_prob = model(new_machine)\n",
    "    \n",
    "    # make it binary...into true or false\n",
    "    predicted_class = (predicted_prob > 0.5).float() \n",
    "    \n",
    "    print(f\"\\nPredicted probability of working for 900 hours of usage: {predicted_prob.item():.4f}\")\n",
    "    print(f\"Predicted class (0=Broken, 1=Working): {predicted_class.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "the-gpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
